{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff157a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from importlib import reload\n",
    "from langdetect import detect\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from dateutil.parser import parse\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from commons import elastic\n",
    "from text_processing import text_normalizer\n",
    "text_normalizer = reload(text_normalizer)\n",
    "from text_processing import duplicate_finder\n",
    "from utilities import excel_writer, excel_reader, utils\n",
    "from text_processing import abbreviations_resolver\n",
    "abbreviations_resolver = reload(abbreviations_resolver)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "_abbreviations_resolver = abbreviations_resolver.AbbreviationsResolver([])\n",
    "_abbreviations_resolver.load_model(\"../model/abbreviations_dicts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccca35",
   "metadata": {},
   "source": [
    "# Check Lab studies and Geo names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b2726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../tmp/combined data.csv: 0.01s\n",
      "Processed file ../tmp/combined data.csv: 0.02s\n"
     ]
    }
   ],
   "source": [
    "df = excel_reader.ExcelReader().read_df_from_excel(\"../tmp/combined data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d94990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"Title\"]\n",
    "df[\"abstract\"] = df[\"Abstract\"]\n",
    "df[\"keywords\"] = \"\"\n",
    "df[\"identificators\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587d4685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lab studies              102\n",
       "Included (abstract)      100\n",
       "Non-causal studies       100\n",
       "High-income countries    100\n",
       "No intervention          100\n",
       "Name: File, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"File\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a68954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 articles\n",
      "Processed 501 articles\n",
      "Processed 0 abbreviations\n",
      "Processed 3000 abbreviations\n",
      "Processed 6000 abbreviations\n",
      "Processed 9000 abbreviations\n",
      "Processed 12000 abbreviations\n",
      "Processed 15000 abbreviations\n",
      "Processed 18000 abbreviations\n",
      "Processed 21000 abbreviations\n",
      "Processed 24000 abbreviations\n",
      "Processed 27000 abbreviations\n",
      "Processed 27129 abbreviations\n"
     ]
    }
   ],
   "source": [
    "from text_processing import search_engine_insensitive_to_spelling\n",
    "from text_processing import all_column_filler\n",
    "search_engine_inverted_index = search_engine_insensitive_to_spelling.SearchEngineInsensitiveToSpelling(\n",
    "        load_abbreviations = True)\n",
    "search_engine_inverted_index.create_inverted_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc501ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing  {'column_filler_class': 'GeoNameFinder'}\n",
      "Read file ../data/provinces.xlsx: 1.53s\n",
      "Processed file ../data/provinces.xlsx: 1.48s\n",
      "Read file ../data/districts.xlsx: 1.65s\n",
      "Processed file ../data/districts.xlsx: 1.58s\n",
      "Processed 0 items\n",
      "Processed 501 items\n",
      "Processed for 60.994033098220825s\n",
      "Started processing  {'column_filler_class': 'StudyTypeLabeller', 'folder': '../model/study_type_multi', 'meta_folder': '../model/study_type_multi_meta_agg', 'scibert_model_folder': '../model/scibert_scivocab_uncased'}\n",
      "Started tokenizer loading\n",
      "Used gpu 0\n",
      "Tokenizrer loaded\n",
      "0\n",
      "../model/scibert_scivocab_uncased\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../model/study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6e33a00b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../model/study_type_multi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6e33a00b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is done\n",
      "INFO:tensorflow:Writing example 0 of 502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] the current study investigated the effectiveness of a group on - line positive psychology intervention ( op ##pi ) designed to mitigate the psychological impact of the cov ##id - 19 pandemic and the subsequent measures to control it . study participants ( n = 82 , m - age = 33 . 07 , sd = 9 . 55 ) were all greek adults divided into an intervention ( n = 44 ) and a control group ( n = 38 ) . the intervention group attended a voluntary , online , two - week , six - session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] the current study investigated the effectiveness of a group on - line positive psychology intervention ( op ##pi ) designed to mitigate the psychological impact of the cov ##id - 19 pandemic and the subsequent measures to control it . study participants ( n = 82 , m - age = 33 . 07 , sd = 9 . 55 ) were all greek adults divided into an intervention ( n = 44 ) and a control group ( n = 38 ) . the intervention group attended a voluntary , online , two - week , six - session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 111 1073 527 3003 111 4826 131 106 583 191 579 972 1532 7517 3832 145 561 3767 546 2933 147 16735 111 6175 2141 131 111 21004 173 579 371 26196 137 111 3766 2554 147 602 256 205 527 1914 145 146 275 8707 422 127 579 1407 275 3307 205 12569 422 3654 275 514 205 4023 546 267 355 22173 4311 4524 690 130 3832 145 146 275 3894 546 137 106 602 583 145 146 275 3357 546 205 111 3832 583 17402 106 13971 422 2997 422 502 579 4082 422 2781 579 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 111 1073 527 3003 111 4826 131 106 583 191 579 972 1532 7517 3832 145 561 3767 546 2933 147 16735 111 6175 2141 131 111 21004 173 579 371 26196 137 111 3766 2554 147 602 256 205 527 1914 145 146 275 8707 422 127 579 1407 275 3307 205 12569 422 3654 275 514 205 4023 546 267 355 22173 4311 4524 690 130 3832 145 146 275 3894 546 137 106 602 583 145 146 275 3357 546 205 111 3832 583 17402 106 13971 422 2997 422 502 579 4082 422 2781 579 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] starting from the report of a project about cap ##oe ##ira that took place at a french school in martin ##ique , this article seeks to describe each step of this experience and analyze its effects on the student body . the project , that was two months long , was based on a communicative , action ##al , and inter ##cultural perspective , and theoretically supported in those areas in the sphere of portuguese as a foreign language ( pfl ) . the analysis encompasses the school ' s and its community ' s socio - cultural context , as well as the social and linguistic aims proposed by the teachers , in favor of a citizen formation . ( english ) [ abstract from author ] a parti ##r do rel ##ato de um proj ##eto sobre a cap ##oe ##ira realiz ##ado em uma esc ##ola na martin ##ica , terr ##itor ##io ins ##ular ultra ##mar ##ino frances , o present ##e arti ##go vis ##a es ##mi ##uca ##r cada et ##apa dess ##a experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] starting from the report of a project about cap ##oe ##ira that took place at a french school in martin ##ique , this article seeks to describe each step of this experience and analyze its effects on the student body . the project , that was two months long , was based on a communicative , action ##al , and inter ##cultural perspective , and theoretically supported in those areas in the sphere of portuguese as a foreign language ( pfl ) . the analysis encompasses the school ' s and its community ' s socio - cultural context , as well as the social and linguistic aims proposed by the teachers , in favor of a citizen formation . ( english ) [ abstract from author ] a parti ##r do rel ##ato de um proj ##eto sobre a cap ##oe ##ira realiz ##ado em uma esc ##ola na martin ##ica , terr ##itor ##io ins ##ular ultra ##mar ##ino frances , o present ##e arti ##go vis ##a es ##mi ##uca ##r cada et ##apa dess ##a experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 4593 263 111 2024 131 106 2575 1011 891 3732 7143 198 7334 2608 235 106 10282 2694 121 9734 2724 422 238 2148 19446 147 3401 535 1371 131 238 2899 137 5230 633 1056 191 111 4570 2160 205 111 2575 422 198 241 502 2499 1113 422 241 791 191 106 25922 422 2517 120 422 137 357 19916 5211 422 137 12138 2810 121 1052 2326 121 111 10121 131 27334 188 106 7543 2647 145 23414 546 205 111 669 23460 111 2694 2505 112 137 633 2928 2505 112 8001 579 6656 2220 422 188 804 188 111 1748 137 9096 7415 1337 214 111 7581 422 121 6466 131 106 24070 2256 205 145 6170 546 260 4940 263 2323 1901 106 1947 30114 572 448 5483 223 3472 3881 9037 22449 106 891 3732 7143 24772 5819 562 17926 6339 8044 2431 9734 3961 422 8177 1602 1450 722 458 10186 8451 2218 30073 422 116 709 30107 5425 2366 1258 30110 601 5526 28382 30114 18553 365 19631 29402 30110 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 4593 263 111 2024 131 106 2575 1011 891 3732 7143 198 7334 2608 235 106 10282 2694 121 9734 2724 422 238 2148 19446 147 3401 535 1371 131 238 2899 137 5230 633 1056 191 111 4570 2160 205 111 2575 422 198 241 502 2499 1113 422 241 791 191 106 25922 422 2517 120 422 137 357 19916 5211 422 137 12138 2810 121 1052 2326 121 111 10121 131 27334 188 106 7543 2647 145 23414 546 205 111 669 23460 111 2694 2505 112 137 633 2928 2505 112 8001 579 6656 2220 422 188 804 188 111 1748 137 9096 7415 1337 214 111 7581 422 121 6466 131 106 24070 2256 205 145 6170 546 260 4940 263 2323 1901 106 1947 30114 572 448 5483 223 3472 3881 9037 22449 106 891 3732 7143 24772 5819 562 17926 6339 8044 2431 9734 3961 422 8177 1602 1450 722 458 10186 8451 2218 30073 422 116 709 30107 5425 2366 1258 30110 601 5526 28382 30114 18553 365 19631 29402 30110 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] breakthrough bleeding episodes at minimum and improvement in quality of life in a child with severe hem ##ophilia a with inhibitors treated with em ##ici ##zu ##ma ##b : a case report from chile . [SEP] objective : rare disease . background : people with hem ##ophilia a have shown osteo ##mus ##cu ##lar complications that have a significant impact on their quality of life ( qol ) and on health care costs . patients with hem ##ophilia a with inhibitors living in developing countries such as chile face a high disease and treatment burden . em ##ici ##zu ##ma ##b , a human ##ized bis ##pecific monoclonal antibody , is associated with improvements in qol and reduction in the financial impact of the disease related to treatment . this case report describes the impact of em ##ici ##zu ##ma ##b on a patient with severe hem ##ophilia a with inhibitors in terms of breakthrough bleeding control , improvements in qol , and reduced financial impact after a year of treatment , in a country where this medication is not routinely available . case report : a 10 - year - old child with severe hem ##ophilia a with inhibitors had several restrictions in his daily life due to multiple incidence ##s of breakthrough bleeding . he was on episodic treatment with bypass ##ing agents , and in the year prior to treatment with em ##ici ##zu ##ma ##b he had 18 bleeding episodes . after 1 year on prophylaxis treatment with em ##ici [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] breakthrough bleeding episodes at minimum and improvement in quality of life in a child with severe hem ##ophilia a with inhibitors treated with em ##ici ##zu ##ma ##b : a case report from chile . [SEP] objective : rare disease . background : people with hem ##ophilia a have shown osteo ##mus ##cu ##lar complications that have a significant impact on their quality of life ( qol ) and on health care costs . patients with hem ##ophilia a with inhibitors living in developing countries such as chile face a high disease and treatment burden . em ##ici ##zu ##ma ##b , a human ##ized bis ##pecific monoclonal antibody , is associated with improvements in qol and reduction in the financial impact of the disease related to treatment . this case report describes the impact of em ##ici ##zu ##ma ##b on a patient with severe hem ##ophilia a with inhibitors in terms of breakthrough bleeding control , improvements in qol , and reduced financial impact after a year of treatment , in a country where this medication is not routinely available . case report : a 10 - year - old child with severe hem ##ophilia a with inhibitors had several restrictions in his daily life due to multiple incidence ##s of breakthrough bleeding . he was on episodic treatment with bypass ##ing agents , and in the year prior to treatment with em ##ici ##zu ##ma ##b he had 18 bleeding episodes . after 1 year on prophylaxis treatment with em ##ici [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23677 8401 10761 235 3142 137 3523 121 1671 131 1994 121 106 1326 190 3167 2372 21508 106 190 5241 2338 190 562 679 12837 903 30125 862 106 820 2024 263 19327 205 103 3201 862 4346 1288 205 2740 862 2325 190 2372 21508 106 360 817 17438 12141 8594 13367 4929 198 360 106 684 2141 191 547 1671 131 1994 145 15971 546 137 191 947 1459 3266 205 568 190 2372 21508 106 190 5241 5404 121 3775 3198 555 188 19327 3942 106 597 1288 137 922 7590 205 562 679 12837 903 30125 422 106 1168 645 5742 9705 9743 3582 422 165 1111 190 7017 121 15971 137 2135 121 111 4437 2141 131 111 1288 1482 147 922 205 238 820 2024 5223 111 2141 131 562 679 12837 903 30125 191 106 1454 190 3167 2372 21508 106 190 5241 121 1615 131 23677 8401 602 422 7017 121 15971 422 137 1797 4437 2141 647 106 996 131 922 422 121 106 5192 582 238 7297 165 302 13423 1427 205 820 2024 862 106 566 579 996 579 4289 1326 190 3167 2372 21508 106 190 5241 883 1323 9129 121 1972 4122 1994 1074 147 1624 3970 30113 131 23677 8401 205 299 241 191 22458 922 190 11403 140 3027 422 137 121 111 996 1979 147 922 190 562 679 12837 903 30125 299 883 1178 8401 10761 205 647 158 996 191 15897 922 190 562 679 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23677 8401 10761 235 3142 137 3523 121 1671 131 1994 121 106 1326 190 3167 2372 21508 106 190 5241 2338 190 562 679 12837 903 30125 862 106 820 2024 263 19327 205 103 3201 862 4346 1288 205 2740 862 2325 190 2372 21508 106 360 817 17438 12141 8594 13367 4929 198 360 106 684 2141 191 547 1671 131 1994 145 15971 546 137 191 947 1459 3266 205 568 190 2372 21508 106 190 5241 5404 121 3775 3198 555 188 19327 3942 106 597 1288 137 922 7590 205 562 679 12837 903 30125 422 106 1168 645 5742 9705 9743 3582 422 165 1111 190 7017 121 15971 137 2135 121 111 4437 2141 131 111 1288 1482 147 922 205 238 820 2024 5223 111 2141 131 562 679 12837 903 30125 191 106 1454 190 3167 2372 21508 106 190 5241 121 1615 131 23677 8401 602 422 7017 121 15971 422 137 1797 4437 2141 647 106 996 131 922 422 121 106 5192 582 238 7297 165 302 13423 1427 205 820 2024 862 106 566 579 996 579 4289 1326 190 3167 2372 21508 106 190 5241 883 1323 9129 121 1972 4122 1994 1074 147 1624 3970 30113 131 23677 8401 205 299 241 191 22458 922 190 11403 140 3027 422 137 121 111 996 1979 147 922 190 562 679 12837 903 30125 299 883 1178 8401 10761 205 647 158 996 191 15897 922 190 562 679 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] how do large commercial banks adjust capital ratios : empirical evidence from the us ? [SEP] this research explores the balanced panel data to examine the level of capital adjustment for major insure ##d commercial banks over the 2002 - 2018 period using a two - step gmm estimator . the findings show that the speed of adjustment of the large insure ##d commercial banks is faster than that of non - financial companies . the results contribute to a slower average adjustment pace of a total capital ratio than the total risk - based capital and capital buffer ratios . the adjustment of capital is faster in the post - crisis period than during and before - crises era . the adequately capital ##ized banks adjust capital ratio faster than well - capital ##ized banks . in contrast , the under - capital ##ized banks adjust the total risk - based capital ratio and capital buffer ratio more quickly than that of others . the low liquid banks needed a higher time to restore equilibrium than high liquid banks . the results of this study have economic significance for policy implications and future regulations . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] how do large commercial banks adjust capital ratios : empirical evidence from the us ? [SEP] this research explores the balanced panel data to examine the level of capital adjustment for major insure ##d commercial banks over the 2002 - 2018 period using a two - step gmm estimator . the findings show that the speed of adjustment of the large insure ##d commercial banks is faster than that of non - financial companies . the results contribute to a slower average adjustment pace of a total capital ratio than the total risk - based capital and capital buffer ratios . the adjustment of capital is faster in the post - crisis period than during and before - crises era . the adequately capital ##ized banks adjust capital ratio faster than well - capital ##ized banks . in contrast , the under - capital ##ized banks adjust the total risk - based capital ratio and capital buffer ratio more quickly than that of others . the low liquid banks needed a higher time to restore equilibrium than high liquid banks . the results of this study have economic significance for policy implications and future regulations . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 539 572 1135 5046 9679 4805 5153 5269 862 4808 1775 263 111 227 3912 103 238 849 20247 111 10064 4864 453 147 4423 111 615 131 5153 7558 168 1626 22393 30118 5046 9679 573 111 8063 579 7491 1275 487 106 502 579 1371 26383 9229 205 111 2116 405 198 111 2747 131 7558 131 111 1135 22393 30118 5046 9679 165 6084 506 198 131 699 579 4437 6724 205 111 545 4362 147 106 10700 1568 7558 19153 131 106 1114 5153 1857 506 111 1114 1265 579 791 5153 137 5153 3520 5269 205 111 7558 131 5153 165 6084 121 111 1422 579 11491 1275 506 781 137 1548 579 28285 12031 205 111 13463 5153 645 9679 4805 5153 1857 6084 506 804 579 5153 645 9679 205 121 2144 422 111 604 579 5153 645 9679 4805 111 1114 1265 579 791 5153 1857 137 5153 3520 1857 475 7493 506 198 131 2968 205 111 629 4401 9679 2764 106 1001 532 147 16295 4211 506 597 4401 9679 205 111 545 131 238 527 360 3587 4150 168 2951 5214 137 2158 12187 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 539 572 1135 5046 9679 4805 5153 5269 862 4808 1775 263 111 227 3912 103 238 849 20247 111 10064 4864 453 147 4423 111 615 131 5153 7558 168 1626 22393 30118 5046 9679 573 111 8063 579 7491 1275 487 106 502 579 1371 26383 9229 205 111 2116 405 198 111 2747 131 7558 131 111 1135 22393 30118 5046 9679 165 6084 506 198 131 699 579 4437 6724 205 111 545 4362 147 106 10700 1568 7558 19153 131 106 1114 5153 1857 506 111 1114 1265 579 791 5153 137 5153 3520 5269 205 111 7558 131 5153 165 6084 121 111 1422 579 11491 1275 506 781 137 1548 579 28285 12031 205 111 13463 5153 645 9679 4805 5153 1857 6084 506 804 579 5153 645 9679 205 121 2144 422 111 604 579 5153 645 9679 4805 111 1114 1265 579 791 5153 1857 137 5153 3520 1857 475 7493 506 198 131 2968 205 111 629 4401 9679 2764 106 1001 532 147 16295 4211 506 597 4401 9679 205 111 545 131 238 527 360 3587 4150 168 2951 5214 137 2158 12187 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] analyzing the effects of gas - to - liquid ( gt ##l ) diesel blend ##ing on the efficiency and emissions of petroleum refine ##ries and transport fuels in the us and europe [SEP] the transport sector is fast changing with demand for dist ##illa ##tes increasing ami ##ds ##t declining gas ##oline consumption in many markets especially in the developed world . increasingly refine ##ries are stretched to operate less efficiently and this is manifested through a drop in efficiency as a consequence of increasing diesel production via less efficient routes , particularly on the marginal barrel of diesel . it has been suggested that this decline in diesel production efficiency , as the ratio of gas ##oline to diesel ( g / d ) production drops , can partly be mitigate ##d through the use of gas - to liquid ( gt ##l ) diesel . in this paper we adopted refine ##ry linear - programming models to represent the refining system in europe as well as a district in the u . s . to investigate the effects of increased availability of gt ##l diesel to a refine ##r on the energy efficiency and gh ##g emissions of refine ##ries . here we showed that indeed there is an improvement in diesel production efficiency with increasing gt ##l concentrations , but this efficiency gain ( < 0 . 5 % ) is insufficient to counter ##act the higher energy consumption and emissions associated with the production of gt ##l , thus [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] analyzing the effects of gas - to - liquid ( gt ##l ) diesel blend ##ing on the efficiency and emissions of petroleum refine ##ries and transport fuels in the us and europe [SEP] the transport sector is fast changing with demand for dist ##illa ##tes increasing ami ##ds ##t declining gas ##oline consumption in many markets especially in the developed world . increasingly refine ##ries are stretched to operate less efficiently and this is manifested through a drop in efficiency as a consequence of increasing diesel production via less efficient routes , particularly on the marginal barrel of diesel . it has been suggested that this decline in diesel production efficiency , as the ratio of gas ##oline to diesel ( g / d ) production drops , can partly be mitigate ##d through the use of gas - to liquid ( gt ##l ) diesel . in this paper we adopted refine ##ry linear - programming models to represent the refining system in europe as well as a district in the u . s . to investigate the effects of increased availability of gt ##l diesel to a refine ##r on the energy efficiency and gh ##g emissions of refine ##ries . here we showed that indeed there is an improvement in diesel production efficiency with increasing gt ##l concentrations , but this efficiency gain ( < 0 . 5 % ) is insufficient to counter ##act the higher energy consumption and emissions associated with the production of gt ##l , thus [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7894 111 1056 131 2704 579 147 579 4401 145 10545 30115 546 18483 22988 140 191 111 2748 137 8050 131 21932 20062 1971 137 3152 21532 121 111 227 137 3371 103 111 3152 5672 165 3254 5468 190 3880 168 553 10054 1813 1953 20901 1591 30108 20653 2704 9936 3337 121 1164 7838 2825 121 111 1815 2399 205 7484 20062 1971 220 26946 147 8915 1279 7132 137 238 165 20462 833 106 4975 121 2748 188 106 5636 131 1953 18483 1865 2168 1279 3316 11567 422 3220 191 111 7468 28894 131 18483 205 256 434 528 2995 198 238 6929 121 18483 1865 2748 422 188 111 1857 131 2704 9936 147 18483 145 159 1352 128 546 1865 12976 422 300 9632 195 16735 30118 833 111 626 131 2704 579 147 4401 145 10545 30115 546 18483 205 121 238 1203 185 6018 20062 1212 1724 579 4886 1262 147 1032 111 29425 429 121 3371 188 804 188 106 10689 121 111 504 205 112 205 147 3782 111 1056 131 1175 5232 131 10545 30115 18483 147 106 20062 30114 191 111 1333 2748 137 5357 30123 8050 131 20062 1971 205 1530 185 1367 198 3462 461 165 130 3523 121 18483 1865 2748 190 1953 10545 30115 2347 422 563 238 2748 3571 145 962 244 205 305 1863 546 165 10119 147 4227 353 111 1001 1333 3337 137 8050 1111 190 111 1865 131 10545 30115 422 959 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7894 111 1056 131 2704 579 147 579 4401 145 10545 30115 546 18483 22988 140 191 111 2748 137 8050 131 21932 20062 1971 137 3152 21532 121 111 227 137 3371 103 111 3152 5672 165 3254 5468 190 3880 168 553 10054 1813 1953 20901 1591 30108 20653 2704 9936 3337 121 1164 7838 2825 121 111 1815 2399 205 7484 20062 1971 220 26946 147 8915 1279 7132 137 238 165 20462 833 106 4975 121 2748 188 106 5636 131 1953 18483 1865 2168 1279 3316 11567 422 3220 191 111 7468 28894 131 18483 205 256 434 528 2995 198 238 6929 121 18483 1865 2748 422 188 111 1857 131 2704 9936 147 18483 145 159 1352 128 546 1865 12976 422 300 9632 195 16735 30118 833 111 626 131 2704 579 147 4401 145 10545 30115 546 18483 205 121 238 1203 185 6018 20062 1212 1724 579 4886 1262 147 1032 111 29425 429 121 3371 188 804 188 106 10689 121 111 504 205 112 205 147 3782 111 1056 131 1175 5232 131 10545 30115 18483 147 106 20062 30114 191 111 1333 2748 137 5357 30123 8050 131 20062 1971 205 1530 185 1367 198 3462 461 165 130 3523 121 18483 1865 2748 190 1953 10545 30115 2347 422 563 238 2748 3571 145 962 244 205 305 1863 546 165 10119 147 4227 353 111 1001 1333 3337 137 8050 1111 190 111 1865 131 10545 30115 422 959 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] the current study investigated the effectiveness of a group on - line positive psychology intervention ( op ##pi ) designed to mitigate the psychological impact of the cov ##id - 19 pandemic and the subsequent measures to control it . study participants ( n = 82 , m - age = 33 . 07 , sd = 9 . 55 ) were all greek adults divided into an intervention ( n = 44 ) and a control group ( n = 38 ) . the intervention group attended a voluntary , online , two - week , six - session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] the current study investigated the effectiveness of a group on - line positive psychology intervention ( op ##pi ) designed to mitigate the psychological impact of the cov ##id - 19 pandemic and the subsequent measures to control it . study participants ( n = 82 , m - age = 33 . 07 , sd = 9 . 55 ) were all greek adults divided into an intervention ( n = 44 ) and a control group ( n = 38 ) . the intervention group attended a voluntary , online , two - week , six - session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 111 1073 527 3003 111 4826 131 106 583 191 579 972 1532 7517 3832 145 561 3767 546 2933 147 16735 111 6175 2141 131 111 21004 173 579 371 26196 137 111 3766 2554 147 602 256 205 527 1914 145 146 275 8707 422 127 579 1407 275 3307 205 12569 422 3654 275 514 205 4023 546 267 355 22173 4311 4524 690 130 3832 145 146 275 3894 546 137 106 602 583 145 146 275 3357 546 205 111 3832 583 17402 106 13971 422 2997 422 502 579 4082 422 2781 579 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 111 1073 527 3003 111 4826 131 106 583 191 579 972 1532 7517 3832 145 561 3767 546 2933 147 16735 111 6175 2141 131 111 21004 173 579 371 26196 137 111 3766 2554 147 602 256 205 527 1914 145 146 275 8707 422 127 579 1407 275 3307 205 12569 422 3654 275 514 205 4023 546 267 355 22173 4311 4524 690 130 3832 145 146 275 3894 546 137 106 602 583 145 146 275 3357 546 205 111 3832 583 17402 106 13971 422 2997 422 502 579 4082 422 2781 579 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine its effects , and two weeks later to examine its long - term effectiveness . the intervention was found to be effective in allevi ##ating the impact of the pandemic and in strengthening participants ' resilience . more specifically , the results showed significant decreases for the intervention group in all measures of psychosocial distress ( anxiety , depression , lon ##eliness and fear ) and significant increases in empathy , resilience , and experience of positive emotions . the study ' s implications for the development and implementation of online psychological interventions during a crisis are discussed . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine its effects , and two weeks later to examine its long - term effectiveness . the intervention was found to be effective in allevi ##ating the impact of the pandemic and in strengthening participants ' resilience . more specifically , the results showed significant decreases for the intervention group in all measures of psychosocial distress ( anxiety , depression , lon ##eliness and fear ) and significant increases in empathy , resilience , and experience of positive emotions . the study ' s implications for the development and implementation of online psychological interventions during a crisis are discussed . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 633 1056 422 137 502 3272 2269 147 4423 633 1113 579 902 4826 205 111 3832 241 797 147 195 2115 121 13172 560 111 2141 131 111 26196 137 121 19909 1914 2505 16865 205 475 3697 422 111 545 1367 684 4819 168 111 3832 583 121 355 2554 131 14951 10581 145 6319 422 4754 422 14194 26699 137 8908 546 137 684 2348 121 23819 422 16865 422 137 2899 131 1532 11394 205 111 527 2505 112 5214 168 111 1120 137 2848 131 2997 6175 5434 781 106 11491 220 2991 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 633 1056 422 137 502 3272 2269 147 4423 633 1113 579 902 4826 205 111 3832 241 797 147 195 2115 121 13172 560 111 2141 131 111 26196 137 121 19909 1914 2505 16865 205 475 3697 422 111 545 1367 684 4819 168 111 3832 583 121 355 2554 131 14951 10581 145 6319 422 4754 422 14194 26699 137 8908 546 137 684 2348 121 23819 422 16865 422 137 2899 131 1532 11394 205 111 527 2505 112 5214 168 111 1120 137 2848 131 2997 6175 5434 781 106 11491 220 2991 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] starting from the report of a project about cap ##oe ##ira that took place at a french school in martin ##ique , this article seeks to describe each step of this experience and analyze its effects on the student body . the project , that was two months long , was based on a communicative , action ##al , and inter ##cultural perspective , and theoretically supported in those areas in the sphere of portuguese as a foreign language ( pfl ) . the analysis encompasses the school ' s and its community ' s socio - cultural context , as well as the social and linguistic aims proposed by the teachers , in favor of a citizen formation . ( english ) [ abstract from author ] a parti ##r do rel ##ato de um proj ##eto sobre a cap ##oe ##ira realiz ##ado em uma esc ##ola na martin ##ica , terr ##itor ##io ins ##ular ultra ##mar ##ino frances , o present ##e arti ##go vis ##a es ##mi ##uca ##r cada et ##apa dess ##a experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] starting from the report of a project about cap ##oe ##ira that took place at a french school in martin ##ique , this article seeks to describe each step of this experience and analyze its effects on the student body . the project , that was two months long , was based on a communicative , action ##al , and inter ##cultural perspective , and theoretically supported in those areas in the sphere of portuguese as a foreign language ( pfl ) . the analysis encompasses the school ' s and its community ' s socio - cultural context , as well as the social and linguistic aims proposed by the teachers , in favor of a citizen formation . ( english ) [ abstract from author ] a parti ##r do rel ##ato de um proj ##eto sobre a cap ##oe ##ira realiz ##ado em uma esc ##ola na martin ##ica , terr ##itor ##io ins ##ular ultra ##mar ##ino frances , o present ##e arti ##go vis ##a es ##mi ##uca ##r cada et ##apa dess ##a experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 4593 263 111 2024 131 106 2575 1011 891 3732 7143 198 7334 2608 235 106 10282 2694 121 9734 2724 422 238 2148 19446 147 3401 535 1371 131 238 2899 137 5230 633 1056 191 111 4570 2160 205 111 2575 422 198 241 502 2499 1113 422 241 791 191 106 25922 422 2517 120 422 137 357 19916 5211 422 137 12138 2810 121 1052 2326 121 111 10121 131 27334 188 106 7543 2647 145 23414 546 205 111 669 23460 111 2694 2505 112 137 633 2928 2505 112 8001 579 6656 2220 422 188 804 188 111 1748 137 9096 7415 1337 214 111 7581 422 121 6466 131 106 24070 2256 205 145 6170 546 260 4940 263 2323 1901 106 1947 30114 572 448 5483 223 3472 3881 9037 22449 106 891 3732 7143 24772 5819 562 17926 6339 8044 2431 9734 3961 422 8177 1602 1450 722 458 10186 8451 2218 30073 422 116 709 30107 5425 2366 1258 30110 601 5526 28382 30114 18553 365 19631 29402 30110 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 4593 263 111 2024 131 106 2575 1011 891 3732 7143 198 7334 2608 235 106 10282 2694 121 9734 2724 422 238 2148 19446 147 3401 535 1371 131 238 2899 137 5230 633 1056 191 111 4570 2160 205 111 2575 422 198 241 502 2499 1113 422 241 791 191 106 25922 422 2517 120 422 137 357 19916 5211 422 137 12138 2810 121 1052 2326 121 111 10121 131 27334 188 106 7543 2647 145 23414 546 205 111 669 23460 111 2694 2505 112 137 633 2928 2505 112 8001 579 6656 2220 422 188 804 188 111 1748 137 9096 7415 1337 214 111 7581 422 121 6466 131 106 24070 2256 205 145 6170 546 260 4940 263 2323 1901 106 1947 30114 572 448 5483 223 3472 3881 9037 22449 106 891 3732 7143 24772 5819 562 17926 6339 8044 2431 9734 3961 422 8177 1602 1450 722 458 10186 8451 2218 30073 422 116 709 30107 5425 2366 1258 30110 601 5526 28382 30114 18553 365 19631 29402 30110 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou com o apo ##rt ##e te ##oric ##o dess ##as areas no ambi ##to do ens ##ino de portugues en ##quant ##o lingu ##a estr ##ange ##ira ( ple ) . a anal ##ise bus ##ca engl ##obar o context ##o socio ##cultural da esc ##ola e do public ##o e os obje ##tivo ##s linguistic ##os e soci ##ais propos ##tos pel ##os doc ##entes , em prol de uma forma ##ca ##o a cid ##ada ##nia . ( portuguese ) [ abstract from author ] copyright of lingu ##agem e ens ##ino is the property of lingu ##agem e ens ##ino and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] experi ##encia e anal ##isa ##r se ##us ef ##eit ##os sobre o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou com o apo ##rt ##e te ##oric ##o dess ##as areas no ambi ##to do ens ##ino de portugues en ##quant ##o lingu ##a estr ##ange ##ira ( ple ) . a anal ##ise bus ##ca engl ##obar o context ##o socio ##cultural da esc ##ola e do public ##o e os obje ##tivo ##s linguistic ##os e soci ##ais propos ##tos pel ##os doc ##entes , em prol de uma forma ##ca ##o a cid ##ada ##nia . ( portuguese ) [ abstract from author ] copyright of lingu ##agem e ens ##ino is the property of lingu ##agem e ens ##ino and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 209 116 20497 5233 30107 1094 21911 30112 29402 142 2326 425 9061 861 572 2658 2218 223 26266 279 25895 30112 8589 30110 13343 713 7143 145 4641 546 205 106 438 764 2817 1999 15065 25205 116 2220 30112 8001 19916 2762 6339 8044 139 572 1771 30112 139 3581 25636 26402 30113 9096 169 139 2436 12937 924 22013 6286 169 2104 29094 422 562 2692 223 17926 30019 1999 30112 106 23434 3415 11404 205 145 27334 546 260 4940 263 2323 1901 4586 131 8589 28096 139 2658 2218 165 111 3713 131 8589 28096 139 2658 2218 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 1852 14129 139 438 6740 30114 262 153 6431 23360 169 22449 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 209 116 20497 5233 30107 1094 21911 30112 29402 142 2326 425 9061 861 572 2658 2218 223 26266 279 25895 30112 8589 30110 13343 713 7143 145 4641 546 205 106 438 764 2817 1999 15065 25205 116 2220 30112 8001 19916 2762 6339 8044 139 572 1771 30112 139 3581 25636 26402 30113 9096 169 139 2436 12937 924 22013 6286 169 2104 29094 422 562 2692 223 17926 30019 1999 30112 106 23434 3415 11404 205 145 27334 546 260 4940 263 2323 1901 4586 131 8589 28096 139 2658 2218 165 111 3713 131 8589 28096 139 2658 2218 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine its effects , and two weeks later to examine its long - term effectiveness . the intervention was found to be effective in allevi ##ating the impact of the pandemic and in strengthening participants ' resilience . more specifically , the results showed significant decreases for the intervention group in all measures of psychosocial distress ( anxiety , depression , lon ##eliness and fear ) and significant increases in empathy , resilience , and experience of positive emotions . the study ' s implications for the development and implementation of online psychological interventions during a crisis are discussed . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] \" stay ##ing home - feeling positive \" : effectiveness of an on - line positive psychology group intervention during the cov ##id - 19 pandemic [SEP] session ( each 50 min ) , group intervention . the intervention aimed at enhancing participants ' personal strengths and resilience in order to cope more effectively with the psychological impact of social dist ##ancing ( e . g . , feelings of anxiety , sad ##ness , fear , and / or lon ##eliness ) . all participants completed an online questionnaire one week before the intervention ' s implementation , which included scales measuring their : demographic characteristics , empathy , resilience , affect ##ivity , feelings of lon ##eliness , depression and anxiety levels , and feelings of fear regarding the outbreak . participants in both the intervention and control group completed the same measures the week following the intervention ' s termination to examine its effects , and two weeks later to examine its long - term effectiveness . the intervention was found to be effective in allevi ##ating the impact of the pandemic and in strengthening participants ' resilience . more specifically , the results showed significant decreases for the intervention group in all measures of psychosocial distress ( anxiety , depression , lon ##eliness and fear ) and significant increases in empathy , resilience , and experience of positive emotions . the study ' s implications for the development and implementation of online psychological interventions during a crisis are discussed . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 633 1056 422 137 502 3272 2269 147 4423 633 1113 579 902 4826 205 111 3832 241 797 147 195 2115 121 13172 560 111 2141 131 111 26196 137 121 19909 1914 2505 16865 205 475 3697 422 111 545 1367 684 4819 168 111 3832 583 121 355 2554 131 14951 10581 145 6319 422 4754 422 14194 26699 137 8908 546 137 684 2348 121 23819 422 16865 422 137 2899 131 1532 11394 205 111 527 2505 112 5214 168 111 1120 137 2848 131 2997 6175 5434 781 106 11491 220 2991 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 1554 7868 140 3417 579 14246 1532 1554 862 4826 131 130 191 579 972 1532 7517 583 3832 781 111 21004 173 579 371 26196 103 4205 145 535 1539 678 546 422 583 3832 205 111 3832 7489 235 10786 1914 2505 4026 11756 137 16865 121 993 147 14649 475 5419 190 111 6175 2141 131 1748 553 6019 145 139 205 159 205 422 14070 131 6319 422 10849 1076 422 8908 422 137 1352 234 14194 26699 546 205 355 1914 5343 130 2997 5692 482 4082 1548 111 3832 2505 112 2848 422 334 1936 5690 5373 547 862 8009 2087 422 23819 422 16865 422 2606 16278 422 14070 131 14194 26699 422 4754 137 6319 1049 422 137 14070 131 8908 3560 111 12083 205 1914 121 655 111 3832 137 602 583 5343 111 855 2554 111 4082 982 111 3832 2505 112 11458 147 4423 633 1056 422 137 502 3272 2269 147 4423 633 1113 579 902 4826 205 111 3832 241 797 147 195 2115 121 13172 560 111 2141 131 111 26196 137 121 19909 1914 2505 16865 205 475 3697 422 111 545 1367 684 4819 168 111 3832 583 121 355 2554 131 14951 10581 145 6319 422 4754 422 14194 26699 137 8908 546 137 684 2348 121 23819 422 16865 422 137 2899 131 1532 11394 205 111 527 2505 112 5214 168 111 1120 137 2848 131 2997 6175 5434 781 106 11491 220 2991 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou com o apo ##rt ##e te ##oric ##o dess ##as areas no ambi ##to do ens ##ino de portugues en ##quant ##o lingu ##a estr ##ange ##ira ( ple ) . a anal ##ise bus ##ca engl ##obar o context ##o socio ##cultural da esc ##ola e do public ##o e os obje ##tivo ##s linguistic ##os e soci ##ais propos ##tos pel ##os doc ##entes , em prol de uma forma ##ca ##o a cid ##ada ##nia . ( portuguese ) [ abstract from author ] copyright of lingu ##agem e ens ##ino is the property of lingu ##agem e ens ##ino and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a cap ##oe ##ira no ens ##ino de portugues em esc ##ola franc ##o - carib ##enh ##a . [SEP] o corp ##o disc ##ente . o proj ##eto , que dur ##ou cer ##ca de doi ##s mes ##es , foi emb ##asa ##do em uma perspec ##tiva com ##unic ##ativ ##a , ac ##ional e inter ##cultural , e cont ##ou com o apo ##rt ##e te ##oric ##o dess ##as areas no ambi ##to do ens ##ino de portugues en ##quant ##o lingu ##a estr ##ange ##ira ( ple ) . a anal ##ise bus ##ca engl ##obar o context ##o socio ##cultural da esc ##ola e do public ##o e os obje ##tivo ##s linguistic ##os e soci ##ais propos ##tos pel ##os doc ##entes , em prol de uma forma ##ca ##o a cid ##ada ##nia . ( portuguese ) [ abstract from author ] copyright of lingu ##agem e ens ##ino is the property of lingu ##agem e ens ##ino and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 209 116 20497 5233 30107 1094 21911 30112 29402 142 2326 425 9061 861 572 2658 2218 223 26266 279 25895 30112 8589 30110 13343 713 7143 145 4641 546 205 106 438 764 2817 1999 15065 25205 116 2220 30112 8001 19916 2762 6339 8044 139 572 1771 30112 139 3581 25636 26402 30113 9096 169 139 2436 12937 924 22013 6286 169 2104 29094 422 562 2692 223 17926 30019 1999 30112 106 23434 3415 11404 205 145 27334 546 260 4940 263 2323 1901 4586 131 8589 28096 139 2658 2218 165 111 3713 131 8589 28096 139 2658 2218 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 106 891 3732 7143 425 2658 2218 223 26266 562 6339 8044 9010 30112 579 25347 8169 30110 205 103 116 7241 30112 706 14433 205 116 3881 9037 422 3941 675 3234 1866 1999 223 2624 30113 2914 123 422 22480 12477 18346 2904 562 17926 9551 24736 209 8160 5122 30110 422 222 8237 139 357 19916 422 139 317 3234 209 116 20497 5233 30107 1094 21911 30112 29402 142 2326 425 9061 861 572 2658 2218 223 26266 279 25895 30112 8589 30110 13343 713 7143 145 4641 546 205 106 438 764 2817 1999 15065 25205 116 2220 30112 8001 19916 2762 6339 8044 139 572 1771 30112 139 3581 25636 26402 30113 9096 169 139 2436 12937 924 22013 6286 169 2104 29094 422 562 2692 223 17926 30019 1999 30112 106 23434 3415 11404 205 145 27334 546 260 4940 263 2323 1901 4586 131 8589 28096 139 2658 2218 165 111 3713 131 8589 28096 139 2658 2218 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] breakthrough bleeding episodes at minimum and improvement in quality of life in a child with severe hem ##ophilia a with inhibitors treated with em ##ici ##zu ##ma ##b : a case report from chile . [SEP] pain control ( 5 - point reduction on the visual analogue scale ) , a decrease in the hem ##ophilia joint health score from 39 to 19 , the qol perception increased by 86 % on the standardized haem ##o - qol - kid ##s , and a 70 % reduction in treatment costs versus the costs of episodic treatment with bypass ##ing agents . conclusions : after 1 year of treatment with em ##ici ##zu ##ma ##b , this patient had substantial improvements in the evaluated parameters . further investigations with em ##ici ##zu ##ma ##b are needed to assess its possible effects on public health policies . [ abstract from author ] copyright of american journal of case reports is the property of international scientific literature , inc and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] breakthrough bleeding episodes at minimum and improvement in quality of life in a child with severe hem ##ophilia a with inhibitors treated with em ##ici ##zu ##ma ##b : a case report from chile . [SEP] pain control ( 5 - point reduction on the visual analogue scale ) , a decrease in the hem ##ophilia joint health score from 39 to 19 , the qol perception increased by 86 % on the standardized haem ##o - qol - kid ##s , and a 70 % reduction in treatment costs versus the costs of episodic treatment with bypass ##ing agents . conclusions : after 1 year of treatment with em ##ici ##zu ##ma ##b , this patient had substantial improvements in the evaluated parameters . further investigations with em ##ici ##zu ##ma ##b are needed to assess its possible effects on public health policies . [ abstract from author ] copyright of american journal of case reports is the property of international scientific literature , inc and its content may not be copied or email ##ed to multiple sites or posted to a lists ##erv without the copyright holder ' s express written permission . however , users may print , download , or email articles for individual use . this abstract may be abr ##idge ##d . no warrant ##y is given about the accuracy of the copy . users should refer to the original published version of the material for the full abstract . ( copyright applies to all abstracts . ) [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23677 8401 10761 235 3142 137 3523 121 1671 131 1994 121 106 1326 190 3167 2372 21508 106 190 5241 2338 190 562 679 12837 903 30125 862 106 820 2024 263 19327 205 103 2675 602 145 305 579 1313 2135 191 111 2180 13964 2211 546 422 106 2640 121 111 2372 21508 3746 947 2867 263 4133 147 371 422 111 15971 6028 1175 214 8676 1863 191 111 8614 6805 30112 579 15971 579 4807 30113 422 137 106 3509 1863 2135 121 922 3266 3304 111 3266 131 22458 922 190 11403 140 3027 205 3078 862 647 158 996 131 922 190 562 679 12837 903 30125 422 238 1454 883 5709 7017 121 111 2840 1496 205 911 7485 190 562 679 12837 903 30125 220 2764 147 1285 633 1263 1056 191 1771 947 5691 205 260 4940 263 2323 1901 4586 131 3258 2042 131 820 3578 165 111 3713 131 2565 4184 2501 422 306 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 23677 8401 10761 235 3142 137 3523 121 1671 131 1994 121 106 1326 190 3167 2372 21508 106 190 5241 2338 190 562 679 12837 903 30125 862 106 820 2024 263 19327 205 103 2675 602 145 305 579 1313 2135 191 111 2180 13964 2211 546 422 106 2640 121 111 2372 21508 3746 947 2867 263 4133 147 371 422 111 15971 6028 1175 214 8676 1863 191 111 8614 6805 30112 579 15971 579 4807 30113 422 137 106 3509 1863 2135 121 922 3266 3304 111 3266 131 22458 922 190 11403 140 3027 205 3078 862 647 158 996 131 922 190 562 679 12837 903 30125 422 238 1454 883 5709 7017 121 111 2840 1496 205 911 7485 190 562 679 12837 903 30125 220 2764 147 1285 633 1263 1056 191 1771 947 5691 205 260 4940 263 2323 1901 4586 131 3258 2042 131 820 3578 165 111 3713 131 2565 4184 2501 422 306 137 633 2118 552 302 195 27634 234 7974 119 147 1624 2494 234 21807 147 106 7439 442 1319 111 4586 19347 2505 112 3583 4293 6829 205 694 422 2485 552 12153 422 12922 422 234 7974 6309 168 1161 626 205 238 4940 552 195 17936 6517 30118 205 425 19863 30126 165 906 1011 111 2683 131 111 6919 205 2485 1055 2935 147 111 2592 2611 2541 131 111 1440 168 111 2327 4940 205 145 4586 9203 147 355 17482 205 546 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] how do large commercial banks adjust capital ratios : empirical evidence from the us ? [SEP] this research explores the balanced panel data to examine the level of capital adjustment for major insure ##d commercial banks over the 2002 - 2018 period using a two - step gmm estimator . the findings show that the speed of adjustment of the large insure ##d commercial banks is faster than that of non - financial companies . the results contribute to a slower average adjustment pace of a total capital ratio than the total risk - based capital and capital buffer ratios . the adjustment of capital is faster in the post - crisis period than during and before - crises era . the adequately capital ##ized banks adjust capital ratio faster than well - capital ##ized banks . in contrast , the under - capital ##ized banks adjust the total risk - based capital ratio and capital buffer ratio more quickly than that of others . the low liquid banks needed a higher time to restore equilibrium than high liquid banks . the results of this study have economic significance for policy implications and future regulations . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] how do large commercial banks adjust capital ratios : empirical evidence from the us ? [SEP] this research explores the balanced panel data to examine the level of capital adjustment for major insure ##d commercial banks over the 2002 - 2018 period using a two - step gmm estimator . the findings show that the speed of adjustment of the large insure ##d commercial banks is faster than that of non - financial companies . the results contribute to a slower average adjustment pace of a total capital ratio than the total risk - based capital and capital buffer ratios . the adjustment of capital is faster in the post - crisis period than during and before - crises era . the adequately capital ##ized banks adjust capital ratio faster than well - capital ##ized banks . in contrast , the under - capital ##ized banks adjust the total risk - based capital ratio and capital buffer ratio more quickly than that of others . the low liquid banks needed a higher time to restore equilibrium than high liquid banks . the results of this study have economic significance for policy implications and future regulations . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 539 572 1135 5046 9679 4805 5153 5269 862 4808 1775 263 111 227 3912 103 238 849 20247 111 10064 4864 453 147 4423 111 615 131 5153 7558 168 1626 22393 30118 5046 9679 573 111 8063 579 7491 1275 487 106 502 579 1371 26383 9229 205 111 2116 405 198 111 2747 131 7558 131 111 1135 22393 30118 5046 9679 165 6084 506 198 131 699 579 4437 6724 205 111 545 4362 147 106 10700 1568 7558 19153 131 106 1114 5153 1857 506 111 1114 1265 579 791 5153 137 5153 3520 5269 205 111 7558 131 5153 165 6084 121 111 1422 579 11491 1275 506 781 137 1548 579 28285 12031 205 111 13463 5153 645 9679 4805 5153 1857 6084 506 804 579 5153 645 9679 205 121 2144 422 111 604 579 5153 645 9679 4805 111 1114 1265 579 791 5153 1857 137 5153 3520 1857 475 7493 506 198 131 2968 205 111 629 4401 9679 2764 106 1001 532 147 16295 4211 506 597 4401 9679 205 111 545 131 238 527 360 3587 4150 168 2951 5214 137 2158 12187 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 539 572 1135 5046 9679 4805 5153 5269 862 4808 1775 263 111 227 3912 103 238 849 20247 111 10064 4864 453 147 4423 111 615 131 5153 7558 168 1626 22393 30118 5046 9679 573 111 8063 579 7491 1275 487 106 502 579 1371 26383 9229 205 111 2116 405 198 111 2747 131 7558 131 111 1135 22393 30118 5046 9679 165 6084 506 198 131 699 579 4437 6724 205 111 545 4362 147 106 10700 1568 7558 19153 131 106 1114 5153 1857 506 111 1114 1265 579 791 5153 137 5153 3520 5269 205 111 7558 131 5153 165 6084 121 111 1422 579 11491 1275 506 781 137 1548 579 28285 12031 205 111 13463 5153 645 9679 4805 5153 1857 6084 506 804 579 5153 645 9679 205 121 2144 422 111 604 579 5153 645 9679 4805 111 1114 1265 579 791 5153 1857 137 5153 3520 1857 475 7493 506 198 131 2968 205 111 629 4401 9679 2764 106 1001 532 147 16295 4211 506 597 4401 9679 205 111 545 131 238 527 360 3587 4150 168 2951 5214 137 2158 12187 205 103 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] analyzing the effects of gas - to - liquid ( gt ##l ) diesel blend ##ing on the efficiency and emissions of petroleum refine ##ries and transport fuels in the us and europe [SEP] are stretched to operate less efficiently and this is manifested through a drop in efficiency as a consequence of increasing diesel production via less efficient routes , particularly on the marginal barrel of diesel . it has been suggested that this decline in diesel production efficiency , as the ratio of gas ##oline to diesel ( g / d ) production drops , can partly be mitigate ##d through the use of gas - to liquid ( gt ##l ) diesel . in this paper we adopted refine ##ry linear - programming models to represent the refining system in europe as well as a district in the u . s . to investigate the effects of increased availability of gt ##l diesel to a refine ##r on the energy efficiency and gh ##g emissions of refine ##ries . here we showed that indeed there is an improvement in diesel production efficiency with increasing gt ##l concentrations , but this efficiency gain ( < 0 . 5 % ) is insufficient to counter ##act the higher energy consumption and emissions associated with the production of gt ##l , thus leading to an overall decline in life cycle efficiency ( up to 5 % ) , and an increase in life cycle gh ##g emissions ( up to 2 % ) . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] analyzing the effects of gas - to - liquid ( gt ##l ) diesel blend ##ing on the efficiency and emissions of petroleum refine ##ries and transport fuels in the us and europe [SEP] are stretched to operate less efficiently and this is manifested through a drop in efficiency as a consequence of increasing diesel production via less efficient routes , particularly on the marginal barrel of diesel . it has been suggested that this decline in diesel production efficiency , as the ratio of gas ##oline to diesel ( g / d ) production drops , can partly be mitigate ##d through the use of gas - to liquid ( gt ##l ) diesel . in this paper we adopted refine ##ry linear - programming models to represent the refining system in europe as well as a district in the u . s . to investigate the effects of increased availability of gt ##l diesel to a refine ##r on the energy efficiency and gh ##g emissions of refine ##ries . here we showed that indeed there is an improvement in diesel production efficiency with increasing gt ##l concentrations , but this efficiency gain ( < 0 . 5 % ) is insufficient to counter ##act the higher energy consumption and emissions associated with the production of gt ##l , thus leading to an overall decline in life cycle efficiency ( up to 5 % ) , and an increase in life cycle gh ##g emissions ( up to 2 % ) . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7894 111 1056 131 2704 579 147 579 4401 145 10545 30115 546 18483 22988 140 191 111 2748 137 8050 131 21932 20062 1971 137 3152 21532 121 111 227 137 3371 103 220 26946 147 8915 1279 7132 137 238 165 20462 833 106 4975 121 2748 188 106 5636 131 1953 18483 1865 2168 1279 3316 11567 422 3220 191 111 7468 28894 131 18483 205 256 434 528 2995 198 238 6929 121 18483 1865 2748 422 188 111 1857 131 2704 9936 147 18483 145 159 1352 128 546 1865 12976 422 300 9632 195 16735 30118 833 111 626 131 2704 579 147 4401 145 10545 30115 546 18483 205 121 238 1203 185 6018 20062 1212 1724 579 4886 1262 147 1032 111 29425 429 121 3371 188 804 188 106 10689 121 111 504 205 112 205 147 3782 111 1056 131 1175 5232 131 10545 30115 18483 147 106 20062 30114 191 111 1333 2748 137 5357 30123 8050 131 20062 1971 205 1530 185 1367 198 3462 461 165 130 3523 121 18483 1865 2748 190 1953 10545 30115 2347 422 563 238 2748 3571 145 962 244 205 305 1863 546 165 10119 147 4227 353 111 1001 1333 3337 137 8050 1111 190 111 1865 131 10545 30115 422 959 3593 147 130 2103 6929 121 1994 2859 2748 145 692 147 305 1863 546 422 137 130 1242 121 1994 2859 5357 30123 8050 145 692 147 170 1863 546 205 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 102 7894 111 1056 131 2704 579 147 579 4401 145 10545 30115 546 18483 22988 140 191 111 2748 137 8050 131 21932 20062 1971 137 3152 21532 121 111 227 137 3371 103 220 26946 147 8915 1279 7132 137 238 165 20462 833 106 4975 121 2748 188 106 5636 131 1953 18483 1865 2168 1279 3316 11567 422 3220 191 111 7468 28894 131 18483 205 256 434 528 2995 198 238 6929 121 18483 1865 2748 422 188 111 1857 131 2704 9936 147 18483 145 159 1352 128 546 1865 12976 422 300 9632 195 16735 30118 833 111 626 131 2704 579 147 4401 145 10545 30115 546 18483 205 121 238 1203 185 6018 20062 1212 1724 579 4886 1262 147 1032 111 29425 429 121 3371 188 804 188 106 10689 121 111 504 205 112 205 147 3782 111 1056 131 1175 5232 131 10545 30115 18483 147 106 20062 30114 191 111 1333 2748 137 5357 30123 8050 131 20062 1971 205 1530 185 1367 198 3462 461 165 130 3523 121 18483 1865 2748 190 1953 10545 30115 2347 422 563 238 2748 3571 145 962 244 205 305 1863 546 165 10119 147 4227 353 111 1001 1333 3337 137 8050 1111 190 111 1865 131 10545 30115 422 959 3593 147 130 2103 6929 121 1994 2859 2748 145 692 147 305 1863 546 422 137 130 1242 121 1994 2859 5357 30123 8050 145 692 147 170 1863 546 205 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: [0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used for model gpu 0\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/study_type_multi/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curie/anaconda3/envs/usaid/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed for 47.09088134765625s\n"
     ]
    }
   ],
   "source": [
    "_all_column_filler = all_column_filler.AllColumnFiller()\n",
    "df = _all_column_filler.fill_columns_for_df(\n",
    "        df, search_engine_inverted_index, _abbreviations_resolver, settings_json = {\"columns\":[\n",
    "            {\"column_filler_class\":\"GeoNameFinder\"},\n",
    "            {\"column_filler_class\": \"StudyTypeLabeller\", \"folder\": \"../model/study_type_multi\",\n",
    "             \"meta_folder\": \"../model/study_type_multi_meta_agg\",\n",
    "             \"scibert_model_folder\": \"../model/scibert_scivocab_uncased\"},\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80582bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Saved to ../tmp/combined_data_processed.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_writer.ExcelWriter().save_df_in_excel(df, \"../tmp/combined_data_processed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c78dc",
   "metadata": {},
   "source": [
    "# Lab studies evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534b7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_study_true_vals = []\n",
    "lab_study_true_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] == \"Lab studies\":\n",
    "        lab_study_true_vals.append(1)\n",
    "    else:\n",
    "        lab_study_true_vals.append(0)\n",
    "    if \"Laboratory study\" in df[\"study_type\"].values[i]:\n",
    "        lab_study_true_pred.append(1)\n",
    "    else:\n",
    "        lab_study_true_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af48c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[388  12]\n",
      " [ 70  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90       400\n",
      "           1       0.73      0.31      0.44       102\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       502\n",
      "   macro avg       0.79      0.64      0.67       502\n",
      "weighted avg       0.82      0.84      0.81       502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(lab_study_true_vals, lab_study_true_pred))  \n",
    "print(classification_report(lab_study_true_vals, lab_study_true_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d01104",
   "metadata": {},
   "source": [
    "# High income countries evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0535a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_locations_info_true_vals = []\n",
    "geo_locations_info_pred = []\n",
    "for i in range(len(df)):\n",
    "    if df[\"File\"].values[i] == 'Lab studies':\n",
    "        continue\n",
    "    if df[\"File\"].values[i] == 'High-income countries':\n",
    "        geo_locations_info_true_vals.append(1)\n",
    "    else:\n",
    "        geo_locations_info_true_vals.append(0)\n",
    "    if 'Low-income countries(995$ or less)' in df[\"world_bankdivision_regions\"].values[i] or 'Middle-income countries($996 to $3,895)' in df[\"world_bankdivision_regions\"].values[i]:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    elif len(df[\"world_bankdivision_regions\"].values[i]) == 0:\n",
    "        geo_locations_info_pred.append(0)\n",
    "    else:\n",
    "        geo_locations_info_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8dd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192 108]\n",
      " [ 24  76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.64      0.74       300\n",
      "           1       0.41      0.76      0.54       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       400\n",
      "   macro avg       0.65      0.70      0.64       400\n",
      "weighted avg       0.77      0.67      0.69       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(geo_locations_info_true_vals, geo_locations_info_pred))  \n",
    "print(classification_report(geo_locations_info_true_vals, geo_locations_info_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (usaid)",
   "language": "python",
   "name": "usaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
